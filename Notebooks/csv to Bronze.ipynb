{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df8e9a35-7335-494b-9b6f-1cdd8ad71e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Yes, you can configure an S3 bucket for free using the AWS Free Tier. AWS offers a free tier that includes 5 GB of standard storage, 20,000 GET requests, and 2,000 PUT requests per month for the first 12 months.\n",
    "\n",
    "Here are the steps to create and configure an S3 bucket:\n",
    "\n",
    "Sign in to the AWS Management Console: Go to the AWS Management Console and sign in with your AWS account.\n",
    "\n",
    "Create an S3 Bucket:\n",
    "\n",
    "Navigate to the S3 service.\n",
    "Click on \"Create bucket\".\n",
    "Enter a unique bucket name and select the region.\n",
    "Configure the bucket settings as needed (you can leave the default settings for now).\n",
    "Click \"Create bucket\".\n",
    "Upload Files to the S3 Bucket:\n",
    "\n",
    "Click on the bucket name you just created.\n",
    "Click on \"Upload\" and follow the instructions to upload your CSV files.\n",
    "Set Permissions:\n",
    "\n",
    "Ensure that the bucket has the appropriate permissions for your Databricks environment to access it.\n",
    "You may need to configure an IAM role with the necessary permissions and attach it to your Databricks cluster.\n",
    "Access the S3 Bucket from Databricks:\n",
    "\n",
    "Use the s3a:// protocol to access your S3 bucket in your Databricks notebook.\n",
    "Here is an example of how to read a CSV file from your S3 bucket in Databricks:\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, FloatType\n",
    "\n",
    "# Define the schema for the sales data\n",
    "salesSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType(), True),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"Tax\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV files from S3 into a DataFrame\n",
    "sales = spark.read.format(\"csv\").option(\"header\", \"true\").schema(salesSchema).load(\"s3a://your-bucket-name/Databricks-Sales/Dataset/*.csv\")\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "display(sales.limit(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1854575d-f20e-457e-911f-1559f0df61eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To configure an IAM role for S3 access in Databricks, follow these steps:\n",
    "\n",
    "Create an IAM Role:\n",
    "\n",
    "Sign in to the AWS Management Console and navigate to the IAM service.\n",
    "Click on \"Roles\" and then \"Create role\".\n",
    "Select \"AWS service\" and choose \"EC2\" as the trusted entity.\n",
    "Click \"Next: Permissions\" and attach the necessary policies for S3 access (e.g., AmazonS3FullAccess).\n",
    "Click \"Next: Tags\" (optional) and then \"Next: Review\".\n",
    "Provide a role name and click \"Create role\".\n",
    "Modify the Trust Relationship:\n",
    "\n",
    "After creating the role, go to the role's \"Trust relationships\" tab.\n",
    "Click \"Edit trust relationship\" and update the policy to allow Databricks to assume the role. Use the following JSON, replacing <databricks-account-id> and <external-id> with your Databricks account ID and external ID:\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": \"arn:aws:iam::<databricks-account-id>:role/<databricks-role>\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\",\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"sts:ExternalId\": \"<external-id>\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "Example:\n",
    "Assuming your Databricks account ID is 123456789012 and the role provided by Databricks is databricks-role, the trust relationship JSON would look like this:\n",
    "\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": \"arn:aws:iam::123456789012:role/databricks-role\"\n",
    "      },\n",
    "      \"Action\": \"sts:AssumeRole\",\n",
    "      \"Condition\": {\n",
    "        \"StringEquals\": {\n",
    "          \"sts:ExternalId\": \"123456789012\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Add the IAM Role to Databricks:\n",
    "\n",
    "In the Databricks workspace, go to the \"Admin Console\" and then \"AWS\".\n",
    "Click \"Add Instance Profile\" and enter the ARN of the IAM role you created.\n",
    "Click \"Add\".\n",
    "Attach the IAM Role to a Cluster:\n",
    "\n",
    "When creating or editing a cluster, go to the \"Advanced Options\" and then \"Instance Profiles\".\n",
    "Select the IAM role you added and attach it to the cluster.\n",
    "Access S3 from Databricks:\n",
    "\n",
    "Use the s3a:// protocol to access your S3 bucket in your Databricks notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232b0d93-8c9a-40d2-948c-ce6db5862499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, FloatType\n",
    "\n",
    "salesSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType(), True),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType(), True),\n",
    "    StructField(\"OrderDate\", DateType(), True),\n",
    "    StructField(\"CustomerName\", StringType(), True),\n",
    "    StructField(\"Email\", StringType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"UnitPrice\", FloatType(), True),\n",
    "    StructField(\"Tax\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV files from S3 into a DataFrame\n",
    "sales = spark.read.format(\"csv\").option(\"header\", \"true\").schema(salesSchema).load(\"s3a://selvaformabucket/Sales/*.csv\")\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "display(sales.limit(10))\n",
    "\n",
    "\"\"\"\n",
    "----------------------------------------------------------------------Code in scala\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val salesSchema = StructType(List(\n",
    "     StructField(\"SalesOrderNumber\", StringType),\n",
    "     StructField(\"SalesOrderLineNumber\", IntegerType),\n",
    "     StructField(\"OrderDate\", DateType),\n",
    "     StructField(\"CustomerName\", StringType),\n",
    "     StructField(\"Email\", StringType),\n",
    "     StructField(\"Item\", StringType),\n",
    "     StructField(\"Quantity\", IntegerType),\n",
    "     StructField(\"UnitPrice\", FloatType),\n",
    "     StructField(\"Tax\", FloatType)\n",
    "     ))\n",
    "\n",
    "val sales = spark.read.format(\"csv\").option(\"header\", \"true\").schema(salesSchema).load(\"Databricks-Sales/Dataset/*.csv\")\n",
    "display(sales.head(10)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea73b5df-1b5f-4640-90a9-146b313ddb2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38301f0-13a8-4c26-b2c6-d8de3f72271e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--create the bronze table.\n",
    "--it is a delta table\n",
    "CREATE OR REPLACE TABLE sales.bronze.sales_bronze\n",
    "(\n",
    "    SalesOrderNumber STRING,\n",
    "    SalesOrderLineNumber INT,\n",
    "    OrderDate DATE,\n",
    "    CustomerName STRING,\n",
    "    Email string,\n",
    "    Item STRING,\n",
    "    Quantity INT,\n",
    "    UnitPrice FLOAT,\n",
    "    Tax FLOAT\n",
    ")\n",
    "USING DELTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1756c3d2-22db-4bd8-972e-ad3efa7f77ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Load the Delta table\n",
    "deltaTable = DeltaTable.forName(spark, \"sales.bronze.sales_bronze\")\n",
    "\n",
    "# Perform the merge operation\n",
    "deltaTable.alias(\"sb\").merge(\n",
    "    sales.alias(\"updates\"),\n",
    "    \"sb.SalesOrderNumber = updates.SalesOrderNumber AND sb.OrderDate = updates.OrderDate AND sb.CustomerName = updates.CustomerName AND sb.Item = updates.Item\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n",
    "        \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n",
    "        \"OrderDate\": \"updates.OrderDate\",\n",
    "        \"CustomerName\": \"updates.CustomerName\",\n",
    "        \"Email\": \"updates.Email\",\n",
    "        \"Item\": \"updates.Item\",\n",
    "        \"Quantity\": \"updates.Quantity\",\n",
    "        \"UnitPrice\": \"updates.UnitPrice\",\n",
    "        \"Tax\": \"updates.Tax\"\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n",
    "        \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n",
    "        \"OrderDate\": \"updates.OrderDate\",\n",
    "        \"CustomerName\": \"updates.CustomerName\",\n",
    "        \"Email\": \"updates.Email\",\n",
    "        \"Item\": \"updates.Item\",\n",
    "        \"Quantity\": \"updates.Quantity\",\n",
    "        \"UnitPrice\": \"updates.UnitPrice\",\n",
    "        \"Tax\": \"updates.Tax\"\n",
    "    }\n",
    ").execute()\n",
    "\"\"\"\n",
    "Code in scala\n",
    "import io.delta.tables._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val deltaTable = DeltaTable.forPath(spark, \"Tables/sales_bronze\")\n",
    "\n",
    "deltaTable.as(\"sb\")\n",
    ".merge(sales.as(\"updates\"),\n",
    "        \"sb.SalesOrderNumber = updates.SalesOrderNumber and sb.OrderDate = updates.OrderDate and sb.CustomerName = updates.CustomerName and sb.Item = updates.Item\")\n",
    "        .whenMatched\n",
    "        .updateExpr(\n",
    "            Map(\n",
    "                \"SalesOrderNumber\" -> \"updates.SalesOrderNumber\",\n",
    "                \"SalesOrderLineNumber\" -> \"updates.SalesOrderLineNumber\",\n",
    "                \"OrderDate\" -> \"updates.OrderDate\",\n",
    "                \"CustomerName\" -> \"updates.CustomerName\",\n",
    "                \"Email\" -> \"updates.Email\",\n",
    "                \"Item\" -> \"updates.Item\",\n",
    "                \"Quantity\" -> \"updates.Quantity\",\n",
    "                \"UnitPrice\" -> \"updates.UnitPrice\",\n",
    "                \"Tax\" -> \"updates.Tax\"\n",
    "            )\n",
    "        )\n",
    "        .whenNotMatched\n",
    "        .insertExpr(\n",
    "            Map(\n",
    "                \"SalesOrderNumber\" -> \"updates.SalesOrderNumber\",\n",
    "                \"SalesOrderLineNumber\" -> \"updates.SalesOrderLineNumber\",\n",
    "                \"OrderDate\" -> \"updates.OrderDate\",\n",
    "                \"CustomerName\" -> \"updates.CustomerName\",\n",
    "                \"Email\" -> \"updates.Email\",\n",
    "                \"Item\" -> \"updates.Item\",\n",
    "                \"Quantity\" -> \"updates.Quantity\",\n",
    "                \"UnitPrice\" -> \"updates.UnitPrice\",\n",
    "                \"Tax\" -> \"updates.Tax\"\n",
    "            )\n",
    "        )\n",
    "        .execute()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97aec7d5-4f4d-493c-9933-629139782dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM sales.bronze.sales_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "969014fd-72a6-4de5-9d9b-ab9f22847d62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4771049375146807,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "csv to Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
